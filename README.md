# Hela

**One-tap capture + instant organization.**

A SwiftUI iOS app that uses on-device Vision and Apple Intelligence to automatically organize your photos and notes.

## Requirements

- iOS 18.0+
- Xcode 16.0+
- Swift 5.9+

## üçé Apple Intelligence Integration

Hela uses **Apple's on-device Foundation Models** (announced WWDC 2025) for intelligent photo classification:
- ‚úÖ **On-device** - works offline, ultra-fast
- ‚úÖ **Private** - data never leaves your device  
- ‚úÖ **Free** - no API costs
- ‚úÖ **Smart** - accurate classification powered by Apple's LLM

**Fallback support:** ChatGPT API (optional) ‚Üí Mock classification (always works)

üìñ See [APPLE_INTELLIGENCE.md](APPLE_INTELLIGENCE.md) for details.

---

## Features

- **Library Tab**: Browse all captured items
  - List view with thumbnails from Photos library
  - Search by title, summary, or tags (case-insensitive)
  - Filter by 9 categories
  - Swipe to delete
  - All queries run locally

- **Capture Tab**: One-tap photo capture
  - Select photos from library with PhotosPicker
  - Real Vision analysis (VNRecognizeObjectsRequest + VNRecognizeTextRequest)
  - On-device AI classification using Foundation Models
  - Editable confirmation sheet before saving
  - Collection and quantity tracking
  - Saves to both Core Data and Photos library

- **Notes Tab**: Text note management
  - Create and organize text notes
  - Searchable by title and content
  - Category tagging

## Architecture

### Views
- `MainTabView.swift`: Root tab view (Library, Capture, Notes)
- `CaptureView.swift`: Photo selection with confirmation sheet
- `LibraryView.swift`: Item list with search and category filters
- `ItemDetailView.swift`: Full item details with metadata
- `NotesView.swift`: Note list and creation

### ViewModels
- `CaptureViewModel.swift`: Manages capture flow with editable fields

### Models
- `VisionSummary.swift`: Vision analysis results (objects, OCR, colors)
- `ItemRecord.swift`: AI classification output (title, summary, category, tags, attributes)
- `AnyCodable.swift`: Type-erased Codable for flexible attributes

### Services
- `VisionAnalyzer.swift`: Apple Vision framework integration
  - Object detection with confidence scores
  - OCR text recognition
  - Dominant color extraction (3-5 colors)

- `AIClassifier.swift`: On-device AI classification
  - Foundation Models API (iOS 18+)
  - Structured JSON output
  - 8 categories: general, bag, recipe, receipt, fashion, decor, document, note
  - 3-10 lowercase singular tags
  - Structured attributes

- `PhotoLibraryService.swift`: Photos framework integration
  - Saves to Photos library
  - Returns PHAsset localIdentifier
  - Loads thumbnails and full images

- `NoteImporter.swift`: Text note processing stub
  - Format detection
  - Title extraction
  - Keyword analysis

### Utilities
- `DominantColor.swift`: Color analysis utility
  - HSB-based color categorization
  - Returns 3-5 simple color names

### Data Layer
- `PersistenceController.swift`: Core Data stack management
- `InventoryStore.swift`: CRUD operations for items and notes

## Core Data Model

### Item Entity
- `id`: UUID
- `createdAt`: Date
- `imageLocalId`: String (PHAsset identifier)
- `imageData`: Binary (compressed JPEG)
- `title`: String
- `summary`: String  
- `category`: String (general|bag|recipe|receipt|fashion|decor|document|note)
- `tagsCSV`: String (comma-separated)
- `dominantColorsJSON`: String (JSON array)
- `attributesJSON`: String (JSON dictionary)
- `ocrText`: String (recognized text)
- `collection`: String (optional grouping)
- `quantity`: Int16 (default 1)
- `notes`: String (optional)
- `classification`: String (legacy)

### NoteEntry Entity
- `id`: UUID
- `createdAt`: Date
- `title`: String
- `body`: String
- `category`: String
- `tagsCSV`: String
- `attributesJSON`: String

## Data Flow

1. User selects photo ‚Üí PhotosPicker
2. Vision analysis ‚Üí `VisionSummary` (objects, OCR, colors)
3. AI classification ‚Üí `ItemRecord` (title, summary, category, tags, attributes)
4. Confirmation sheet ‚Üí User edits fields (title, summary, category, tags, collection, quantity)
5. Save to Photos ‚Üí Get PHAsset localIdentifier
6. Save to Core Data ‚Üí Item with all fields
7. Show "Saved" toast
8. Item appears in Library

## Categories

Hela supports 8 item categories:
- **general**: Miscellaneous items
- **bag**: Bags, purses, backpacks
- **recipe**: Recipe cards, food photos
- **receipt**: Receipts, invoices
- **fashion**: Clothing, accessories
- **decor**: Home decor, furniture
- **document**: Documents, papers
- **note**: Text notes, memos

## Privacy & Permissions

- **Camera**: For future camera capture feature
- **Photo Library Access**: To select photos
- **Photo Library Add**: To save analyzed photos

All processing runs on-device. No data leaves your phone.

## Project Structure

```
Hela/
‚îú‚îÄ‚îÄ HelaApp.swift                  # App entry point
‚îú‚îÄ‚îÄ MainTabView.swift              # Tab navigation
‚îú‚îÄ‚îÄ Info.plist                     # Permissions & config
‚îú‚îÄ‚îÄ Views/
‚îÇ   ‚îú‚îÄ‚îÄ CaptureView.swift         # Photo capture & confirmation
‚îÇ   ‚îú‚îÄ‚îÄ LibraryView.swift         # Item list with filters
‚îÇ   ‚îú‚îÄ‚îÄ ItemDetailView.swift     # Item details
‚îÇ   ‚îî‚îÄ‚îÄ NotesView.swift           # Note management
‚îú‚îÄ‚îÄ ViewModels/
‚îÇ   ‚îî‚îÄ‚îÄ CaptureViewModel.swift    # Capture state & logic
‚îú‚îÄ‚îÄ Models/
‚îÇ   ‚îú‚îÄ‚îÄ VisionSummary.swift       # Vision results
‚îÇ   ‚îú‚îÄ‚îÄ ItemRecord.swift          # Classification output
‚îÇ   ‚îî‚îÄ‚îÄ AnyCodable.swift          # Flexible Codable
‚îú‚îÄ‚îÄ Services/
‚îÇ   ‚îú‚îÄ‚îÄ VisionAnalyzer.swift      # Vision framework
‚îÇ   ‚îú‚îÄ‚îÄ AIClassifier.swift        # On-device AI
‚îÇ   ‚îú‚îÄ‚îÄ PhotoLibraryService.swift # Photos integration
‚îÇ   ‚îî‚îÄ‚îÄ NoteImporter.swift        # Note processing
‚îú‚îÄ‚îÄ Utilities/
‚îÇ   ‚îî‚îÄ‚îÄ DominantColor.swift       # Color extraction
‚îî‚îÄ‚îÄ Data/
    ‚îú‚îÄ‚îÄ PersistenceController.swift # Core Data stack
    ‚îú‚îÄ‚îÄ InventoryStore.swift       # Data access layer
    ‚îî‚îÄ‚îÄ Hela.xcdatamodeld/         # Core Data model
```

## Building

No third-party dependencies required. All frameworks are part of iOS SDK:
- SwiftUI
- PhotosUI
- CoreData
- Vision
- CoreML
- Photos

```bash
open Hela.xcodeproj
# Select target device (iOS 18+)
# Build and run
```

## Implementation Status

‚úÖ **Fully Implemented:**
- Vision analysis (objects, OCR, colors)
- Photo library integration
- Confirmation sheet with editable fields
- Core Data with Item and NoteEntry entities
- Search and filtering
- Collection and quantity tracking
- Notes tab with basic CRUD

‚ö†Ô∏è **Stub/Mock:**
- Apple Intelligence API (structure ready, waiting for production API)
- NoteImporter text processing

## Future Enhancements

- Camera capture (currently photo picker only)
- Advanced note parsing with NLP
- Item editing after save
- Data export (CSV, JSON)
- Cloud sync (CloudKit)
- Share extension
- Widgets
- Barcode/QR scanning

## License

Created for demonstration purposes.
